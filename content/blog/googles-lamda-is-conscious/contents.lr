_model: page
---
title: Google's LaMDA is conscious? 

---
body:

Here's a conversation Google Engineer, Blake Lemoine, allegedly had with an AI language system he was working on:

![Screenshot of convo with lamda](lamda-convo.png)

Upon him leaking this information, he was put on leave by the company. He tweeted "Google might call this sharing proprietary property. I call it sharing a discussion that I had with one of my coworkers.” I am not going to mindread and attempt to evaluate how sincere he is in believing this system is <i>actually</i> sentient, because it merely begs the question <i>how do we know when something is conscious?</i> It's a much more troubling question the more you think about it. I can't <i>prove</i> anyone or anything other than myself is conscious. I assume they are,  but I can't prove it – I don't know what would even constitute such a proof. I would somehow have to gain access to one's internal first-person perspective of the world. 

The more something resembles ourselves the more charitable we are in imputing consciousness to it. It's easier to believe that gorillas are conscious than fish, because they behaviorally and anatomically resemble us. If we had a robot that was designed to be as anthropomorphic as we could possibly make it, there would probably be a point at which we'd stop asking whether or not it's conscious and just start treating it as if it was. This is what makes Westworld such an engaging show. The conflict between what our affective emotional mind wants to believe and what our rational mind can logically prove is on full display. 

The responses to Lemoine by experts in the field of artificial intelligence and machine learning were largely dismissive. It's easy to get caught up in all kinds of imaginative science fiction extrapolations. It's much harder to establish definitions and criteria to evaluate an elusive phenomenon like consciousness. I myself am in the dismissive camp, not because I think it's in principle impossible to artificially create a machine endowed with consciousness, but merely because how much more computationally efficient and complicated brains are than even the most advanced computers. The human brain can generalize in a way computers cannot. 

Those dismissive of Lemoine made the "this AI system is <i>merely</i> extracting patterns from data and convinciingly using them to mimic convenrsation." Yes, that is correct, but how is the human brain any different? We have some innate brain structures that nature has selected for – the mechanisms through which volumes upon volumes of information from the outside world are passed throughout our lifetimes, the ready-receptacles for cultural transmission, the hardware equipped with advanced machine learning algorithms primed to pick upup and participate in language.


---
_template: page.html
